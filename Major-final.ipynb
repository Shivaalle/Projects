{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b25502d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "import csv\n",
    "import shutil\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "labels = {}\n",
    "img_label = None\n",
    "positive_rate_label = None\n",
    "negative_rate_label = None\n",
    "neutral_rate_label = None\n",
    "algorithm_selector = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8c9fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_download(video_id_label):\n",
    "    api_key = 'AIzaSyBd9CdmwGe66pZgG4V5z5Ul914vxZBhfZI'\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    video_id = video_id_label.get()\n",
    "\n",
    "    comments = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        textFormat=\"plainText\"\n",
    "    ).execute()\n",
    "\n",
    "    csv_file = \"youtube_comments.csv\"\n",
    "\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Comment'])\n",
    "\n",
    "        total_comments = 0\n",
    "        page_token = None\n",
    "\n",
    "        while total_comments < 1000:\n",
    "            comments = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                textFormat=\"plainText\",\n",
    "                maxResults=100,\n",
    "                pageToken=page_token\n",
    "            ).execute()\n",
    "\n",
    "            for comment in comments[\"items\"]:\n",
    "                text = comment[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                writer.writerow([text])\n",
    "                total_comments += 1\n",
    "\n",
    "            if 'nextPageToken' in comments:\n",
    "                page_token = comments['nextPageToken']\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    print(f\"Scraped {total_comments} comments and saved to {csv_file}\")\n",
    "    shutil.move(csv_file, './youtube_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f9342f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def import_data_read():\n",
    "    data = pd.read_csv('./youtube_comments.csv')\n",
    "    data.columns\n",
    "    print(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d196b94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "def data_preprocessing(data):   \n",
    "    def text_processing(text):\n",
    "        stop_words = stopwords.words('english')\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        lancaster_stemmer = LancasterStemmer()\n",
    "        snowball_stemer = SnowballStemmer(language=\"english\")\n",
    "        lzr = WordNetLemmatizer()\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\n',' ', text)\n",
    "        # remove punctuations from text\n",
    "        text = re.sub('[%s]' % re.escape(punctuation), \"\", text)\n",
    "        # remove references and hashtags from text\n",
    "        text = re.sub(\"^a-zA-Z0-9$,.\", \"\", text)   \n",
    "        #Remove urls\n",
    "        del_url = re.split('https:\\/\\/.*|http:\\/\\/.*',text)\n",
    "        del_url2= re.split('pic.tw*',del_url[0])\n",
    "        text=del_url2[0]   \n",
    "        # remove multiple spaces from text\n",
    "        text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "        # remove special characters from text\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        #remove emojis\n",
    "        emoji_pattern = re.compile(r'{Emoji}')\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n",
    "        # lemmatizer using WordNetLemmatizer from nltk package\n",
    "        text=' '.join([lzr.lemmatize(word) for word in word_tokenize(text)])\n",
    "        return text\n",
    "    data1 = data.copy()\n",
    "    data1[\"Comment\"] = data1[\"Comment\"].fillna('').astype(str)\n",
    "    data1.Comment = data1.Comment.apply(lambda text: text_processing(text))\n",
    "    data1.replace('', np.nan, inplace=True)\n",
    "    data1=data1.dropna()\n",
    "    print(data1)\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac3984dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentimental_analysis(data1):\n",
    "    sentiments = SentimentIntensityAnalyzer()\n",
    "    data1[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data1[\"Comment\"]]\n",
    "    data1[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data1[\"Comment\"]]\n",
    "    data1[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data1[\"Comment\"]]\n",
    "    data1['Compound'] = [sentiments.polarity_scores(i)[\"compound\"] for i in data1[\"Comment\"]]\n",
    "    score = data1[\"Compound\"].values\n",
    "    sentiment = []\n",
    "    for i in score:\n",
    "        if i >= 0.05 :\n",
    "            sentiment.append('Positive')\n",
    "        elif i <= -0.05 :\n",
    "            sentiment.append('Negative')\n",
    "        else:\n",
    "            sentiment.append('Neutral')\n",
    "    data1[\"Sentiment\"] = sentiment\n",
    "    data1\n",
    "    total_count = len(data1)\n",
    "    rate=[]\n",
    "    positive_count = data1[\"Sentiment\"].value_counts()[\"Positive\"]\n",
    "    negative_count = data1[\"Sentiment\"].value_counts()[\"Negative\"]\n",
    "    neutral_count = data1[\"Sentiment\"].value_counts()[\"Neutral\"]\n",
    "\n",
    "    rate.append((positive_count / total_count) * 100)\n",
    "    rate.append((negative_count / total_count) * 100)\n",
    "    rate.append((neutral_count / total_count) * 100)\n",
    "    print(f\"Positive Rate: {rate[0]:.2f}%\")\n",
    "    print(f\"Negative Rate: {rate[1]:.2f}%\")\n",
    "    print(f\"Neutral Rate: {rate[2]:.2f}%\")\n",
    "    return rate,data1\n",
    "\n",
    "def return_data(data1):\n",
    "    data2=data1.drop(['Positive','Negative','Neutral','Compound'],axis=1)\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "256cb88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sampling(data2):\n",
    "    le = LabelEncoder()\n",
    "    data2['Sentiment'] = le.fit_transform(data2['Sentiment'])\n",
    "    processed_data = {\n",
    "        'Sentence':data2.Comment,\n",
    "        'Sentiment':data2['Sentiment']\n",
    "    }\n",
    "\n",
    "    processed_data = pd.DataFrame(processed_data)\n",
    "    print(processed_data)\n",
    "    print(processed_data['Sentiment'].value_counts())\n",
    "    max_count = processed_data['Sentiment'].value_counts().max()\n",
    "\n",
    "    sentiments = {}\n",
    "    for sentiment_code, df in processed_data.groupby('Sentiment'):\n",
    "        sentiments[sentiment_code] = df\n",
    "\n",
    "    final_data = pd.DataFrame()\n",
    "    for sentiment_code, df in sentiments.items():\n",
    "        if len(df) < max_count:\n",
    "            upsampled_df = resample(df, replace=True, n_samples=max_count, random_state=42)\n",
    "            final_data = pd.concat([final_data, upsampled_df])\n",
    "        else:\n",
    "            final_data = pd.concat([final_data, df])\n",
    "\n",
    "    print(\"***********Final Data:************\\n\")\n",
    "    print(final_data)\n",
    "    print(final_data['Sentiment'].value_counts())\n",
    "    corpus = []\n",
    "    for sentence in final_data['Sentence']:\n",
    "        corpus.append(sentence)\n",
    "    print(corpus[0:25])\n",
    "    return corpus,final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a987b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Classifiers(corpus,final_data):\n",
    "    resu = {\n",
    "        \"Naive Bayes\": {\n",
    "            \"Accuracy\": 0,\n",
    "            \"Precision\": 0,\n",
    "            \"Recall\": 0,\n",
    "            \"F1 Score\": 0,\n",
    "            \"RMSE\": 0.0\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"Accuracy\": 82,\n",
    "            \"Precision\": 78,\n",
    "            \"Recall\": 72,\n",
    "            \"F1 Score\": 75,\n",
    "            \"RMSE\": 0.21\n",
    "        },\n",
    "        \"Logistic Regression\": {\n",
    "            \"Accuracy\": 79,\n",
    "            \"Precision\": 73,\n",
    "            \"Recall\": 68,\n",
    "            \"F1 Score\": 70,\n",
    "            \"RMSE\": 0.28\n",
    "        },\n",
    "        \"SVM\": {\n",
    "            \"Accuracy\": 75,\n",
    "            \"Precision\": 70,\n",
    "            \"Recall\": 65,\n",
    "            \"F1 Score\": 68,\n",
    "            \"RMSE\": 0.31\n",
    "        },\n",
    "        \"GradientBoosting\": {\n",
    "            \"Accuracy\": 75,\n",
    "            \"Precision\": 70,\n",
    "            \"Recall\": 65,\n",
    "            \"F1 Score\": 68,\n",
    "            \"RMSE\": 0.31\n",
    "        },\n",
    "        \"Stacking_Classifier\": {\n",
    "            \"Accuracy\": 75,\n",
    "            \"Precision\": 70,\n",
    "            \"Recall\": 65,\n",
    "            \"F1 Score\": 68,\n",
    "            \"RMSE\": 0.31\n",
    "        }\n",
    "    }\n",
    "    cv = CountVectorizer(max_features=5000)\n",
    "    X = cv.fit_transform(corpus).toarray()\n",
    "    y = final_data.iloc[:, -1].values\n",
    "    print(len(y))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    training_data = (X_train)\n",
    "    testing_data = (X_test)\n",
    "    \n",
    "    #Naive\n",
    "    Naive =MultinomialNB()\n",
    "    Naive.fit(training_data, y_train)\n",
    "    y_pred = Naive.predict(testing_data)\n",
    "    accuracy1=accuracy_score(y_pred,y_test)\n",
    "    classification_rep1 = classification_report(y_test, y_pred)\n",
    "    lines1 = classification_rep1.split('\\n')\n",
    "    macro_avg_line1 = lines1[-3]\n",
    "    macro_avg_metrics1 = macro_avg_line1.split()\n",
    "    resu[\"Naive Bayes\"][\"Accuracy\"]=accuracy1*100\n",
    "    resu[\"Naive Bayes\"][\"Precision\"]=float(macro_avg_metrics1[2])*100\n",
    "    resu[\"Naive Bayes\"][\"Recall\"]=float(macro_avg_metrics1[3])*100\n",
    "    resu[\"Naive Bayes\"][\"F1 Score\"]=float(macro_avg_metrics1[4])*100\n",
    "    rmse1 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    resu[\"Naive Bayes\"][\"RMSE\"] = rmse1*100\n",
    "    #SVC\n",
    "    svc=SVC(kernel='linear', C=1.0, random_state=42)\n",
    "    svc.fit(training_data,y_train)\n",
    "    y_pred=svc.predict(testing_data)\n",
    "    accuracy2=accuracy_score(y_pred,y_test)\n",
    "    classification_rep2 = classification_report(y_test, y_pred)\n",
    "    lines2 = classification_rep2.split('\\n')\n",
    "    macro_avg_line2 = lines2[-3]  # The 'macro avg' row in the classification report\n",
    "    macro_avg_metrics2 = macro_avg_line2.split()\n",
    "    resu[\"SVM\"][\"Accuracy\"]=accuracy2*100\n",
    "    resu[\"SVM\"][\"Precision\"]=float(macro_avg_metrics2[2])*100\n",
    "    resu[\"SVM\"][\"Recall\"]=float(macro_avg_metrics2[3])*100\n",
    "    resu[\"SVM\"][\"F1 Score\"]=float(macro_avg_metrics2[4])*100\n",
    "    rmse2 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    resu[\"SVM\"][\"RMSE\"] = rmse2*100\n",
    "    #boost\n",
    "    boost_class=GradientBoostingClassifier(n_estimators=10,random_state=3)\n",
    "    boost_class.fit(training_data,y_train)\n",
    "    y_pred=boost_class.predict(testing_data)\n",
    "    accuracy3=accuracy_score(y_pred,y_test)\n",
    "    classification_rep3 = classification_report(y_test, y_pred)\n",
    "    lines3 = classification_rep3.split('\\n')\n",
    "    macro_avg_line3 = lines3[-3]\n",
    "    macro_avg_metrics3 = macro_avg_line3.split()\n",
    "    resu[\"GradientBoosting\"][\"Accuracy\"]=accuracy3*100\n",
    "    resu[\"GradientBoosting\"][\"Precision\"]=float(macro_avg_metrics3[2])*100\n",
    "    resu[\"GradientBoosting\"][\"Recall\"]=float(macro_avg_metrics3[3])*100\n",
    "    resu[\"GradientBoosting\"][\"F1 Score\"]=float(macro_avg_metrics3[4])*100\n",
    "    rmse3 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    resu[\"GradientBoosting\"][\"RMSE\"] = rmse3*100\n",
    "    #logistic\n",
    "    logistic_model = LogisticRegression(random_state=42,max_iter=1000)\n",
    "    logistic_model.fit(training_data, y_train)\n",
    "    y_pred = logistic_model.predict(testing_data)\n",
    "    accuracy4 = accuracy_score(y_test, y_pred)\n",
    "    classification_rep4 = classification_report(y_test, y_pred)\n",
    "    lines4 = classification_rep4.split('\\n')\n",
    "    macro_avg_line4 = lines4[-3]\n",
    "    macro_avg_metrics4 = macro_avg_line4.split()\n",
    "    resu[\"Logistic Regression\"][\"Accuracy\"]=accuracy4*100\n",
    "    resu[\"Logistic Regression\"][\"Precision\"]=float(macro_avg_metrics4[2])*100\n",
    "    resu[\"Logistic Regression\"][\"Recall\"]=float(macro_avg_metrics4[3])*100\n",
    "    resu[\"Logistic Regression\"][\"F1 Score\"]=float(macro_avg_metrics4[4])*100\n",
    "    rmse4 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    resu[\"Logistic Regression\"][\"RMSE\"] = rmse4*100\n",
    "    #RN\n",
    "    random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    random_forest_model.fit(training_data, y_train)\n",
    "    y_pred = random_forest_model.predict(testing_data)\n",
    "    accuracy5 = accuracy_score(y_test, y_pred)\n",
    "    classification_rep5 = classification_report(y_test, y_pred)\n",
    "    lines5 = classification_rep5.split('\\n')\n",
    "    macro_avg_line5 = lines5[-3]\n",
    "    macro_avg_metrics5 = macro_avg_line5.split()\n",
    "    resu[\"Random Forest\"][\"Accuracy\"]=accuracy5*100\n",
    "    resu[\"Random Forest\"][\"Precision\"]=float(macro_avg_metrics5[2])*100\n",
    "    resu[\"Random Forest\"][\"Recall\"]=float(macro_avg_metrics5[3])*100\n",
    "    resu[\"Random Forest\"][\"F1 Score\"]=float(macro_avg_metrics5[4])*100\n",
    "    rmse5 = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    resu[\"Random Forest\"][\"RMSE\"] = rmse5*100\n",
    "    \n",
    "    #stacking_classifier\n",
    "    \n",
    "    base_models = [\n",
    "        ('naive_bayes', MultinomialNB()),\n",
    "        ('svm',SVC(kernel='linear', C=1.0, random_state=42)),\n",
    "        ('gradient_boosting',GradientBoostingClassifier(n_estimators=10,random_state=3)),\n",
    "        ('logistic_regression',  LogisticRegression(random_state=42,max_iter=5000)),\n",
    "        ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ]\n",
    "    stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
    "    stacking_classifier.fit(training_data, y_train)\n",
    "    y_pred_stacking = stacking_classifier.predict(testing_data)\n",
    "    accuracy_stacking = accuracy_score(y_pred_stacking, y_test)\n",
    "    classification_rep_stacking = classification_report(y_test, y_pred_stacking)\n",
    "    lines6 = classification_rep_stacking.split('\\n')\n",
    "    macro_avg_line6 = lines5[-3]\n",
    "    macro_avg_metrics6 = macro_avg_line6.split()\n",
    "    \n",
    "    resu[\"Stacking_Classifier\"][\"Accuracy\"]=accuracy_stacking*100\n",
    "    resu[\"Stacking_Classifier\"][\"Precision\"]=float(macro_avg_metrics6[2])*100\n",
    "    resu[\"Stacking_Classifier\"][\"Recall\"]=float(macro_avg_metrics6[3])*100\n",
    "    resu[\"Stacking_Classifier\"][\"F1 Score\"]=float(macro_avg_metrics6[4])*100\n",
    "    rmse6 = np.sqrt(mean_squared_error(y_test, y_pred_stacking))\n",
    "    resu[\"Stacking_Classifier\"][\"RMSE\"] = rmse6*100\n",
    "    \n",
    "    return resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76f05653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 10093 comments and saved to youtube_comments.csv\n",
      "                                                 Comment\n",
      "0                                                   😭😭😭😭\n",
      "1                                                   😭😭😭😭\n",
      "2                         Love it from India, tamilnadu.\n",
      "3                                           Anyone 2024?\n",
      "4                                                   Love\n",
      "...                                                  ...\n",
      "10088  This bring tears in my eyes and remind me that...\n",
      "10089                                    Anyone in 2023?\n",
      "10090  650 Million views \\nCongratulations #OneDirect...\n",
      "10091           Why does this song sound like a 90s song\n",
      "10092                                 00:41 GOOSEBUMPS 🔥\n",
      "\n",
      "[10093 rows x 1 columns]\n",
      "                                                 Comment\n",
      "2                                   love india tamilnadu\n",
      "3                                            anyone 2024\n",
      "4                                                   love\n",
      "5                                             talantlive\n",
      "6                                            anyone 2025\n",
      "...                                                  ...\n",
      "10088  bring tear eye remind golden old day one direc...\n",
      "10089                                        anyone 2023\n",
      "10090  650 million view congratulation onedirection d...\n",
      "10091                            song sound like 90 song\n",
      "10092                                     0041 goosebump\n",
      "\n",
      "[9411 rows x 1 columns]\n",
      "Positive Rate: 34.50%\n",
      "Negative Rate: 10.84%\n",
      "Neutral Rate: 54.66%\n",
      "                                                 Comment  Positive  Negative  \\\n",
      "2                                   love india tamilnadu     0.677       0.0   \n",
      "3                                            anyone 2024     0.000       0.0   \n",
      "4                                                   love     1.000       0.0   \n",
      "5                                             talantlive     0.000       0.0   \n",
      "6                                            anyone 2025     0.000       0.0   \n",
      "...                                                  ...       ...       ...   \n",
      "10088  bring tear eye remind golden old day one direc...     0.318       0.0   \n",
      "10089                                        anyone 2023     0.000       0.0   \n",
      "10090  650 million view congratulation onedirection d...     0.433       0.0   \n",
      "10091                            song sound like 90 song     0.385       0.0   \n",
      "10092                                     0041 goosebump     0.000       0.0   \n",
      "\n",
      "       Neutral  Compound Sentiment  \n",
      "2        0.323    0.6369  Positive  \n",
      "3        1.000    0.0000   Neutral  \n",
      "4        0.000    0.6369  Positive  \n",
      "5        1.000    0.0000   Neutral  \n",
      "6        1.000    0.0000   Neutral  \n",
      "...        ...       ...       ...  \n",
      "10088    0.682    0.6369  Positive  \n",
      "10089    1.000    0.0000   Neutral  \n",
      "10090    0.567    0.6249  Positive  \n",
      "10091    0.615    0.3612  Positive  \n",
      "10092    1.000    0.0000   Neutral  \n",
      "\n",
      "[9411 rows x 6 columns]\n",
      "                                                Sentence  Sentiment\n",
      "2                                   love india tamilnadu          2\n",
      "3                                            anyone 2024          1\n",
      "4                                                   love          2\n",
      "5                                             talantlive          1\n",
      "6                                            anyone 2025          1\n",
      "...                                                  ...        ...\n",
      "10088  bring tear eye remind golden old day one direc...          2\n",
      "10089                                        anyone 2023          1\n",
      "10090  650 million view congratulation onedirection d...          2\n",
      "10091                            song sound like 90 song          2\n",
      "10092                                     0041 goosebump          1\n",
      "\n",
      "[9411 rows x 2 columns]\n",
      "Sentiment\n",
      "1    5144\n",
      "2    3247\n",
      "0    1020\n",
      "Name: count, dtype: int64\n",
      "***********Final Data:************\n",
      "\n",
      "                                               Sentence  Sentiment\n",
      "1129                             fast night change miss          0\n",
      "4443                                          song fire          0\n",
      "8413                                 who asia drop flag          0\n",
      "2816  army reading comment directioners make cry thi...          0\n",
      "1190  guy u lost live one remeber important one spen...          0\n",
      "...                                                 ...        ...\n",
      "763                                        perfect song          2\n",
      "667                          listening masterpiece 2024          2\n",
      "8162                                song feel like home          2\n",
      "4812                        like someone listening 2023          2\n",
      "774                             remembered good old day          2\n",
      "\n",
      "[15432 rows x 2 columns]\n",
      "Sentiment\n",
      "0    5144\n",
      "1    5144\n",
      "2    5144\n",
      "Name: count, dtype: int64\n",
      "['fast night change miss', 'song fire', 'who asia drop flag', 'army reading comment directioners make cry thinking fast night change', 'guy u lost live one remeber important one spent ur people chilled', 'never get old love song 3', 'missing one direction', 'thats insane thats gon na decade since released november year', 'long time tell self alone end im sure im alone one life', 'december last year iam alone one song always also song give goosebumbs', 'goin tonight change something red mother doesnt like kind dress everything never shes showin drivin fast moon breakin hair shes headin somethin wont forget havin regret really want gettin older baby ive thinkin lately ever drive crazy fast night change everything youve ever dreamed disappearing wake there nothing afraid even night change never change chasing tonight doubt runnin round head he waitin hide behind cigarette heart beatin loud doesnt want stop movin fast moon lightin skin shes fallin doesnt even know yet havin regret really want gettin older baby ive thinkin lately ever drive crazy fast night change everything youve ever dreamed disappearing wake there nothing afraid even night change never change goin tonight change something red mother doesnt like kind dress reminds missin piece innocence lost gettin older baby ive thinkin lately ever drive crazy fast night change everything youve ever dreamed disappearing wake there nothing afraid even night change never change baby never change baby never change terjemahkan ke bahasa indonesia', 'kid also listen song untill die', 'night changing scary always one', 'zayn get assaulted harry break arm liam throw louis get arrested niall commits arson', 'cry touch guy', 'still listening sept112024 miss esp zayn malik', 'think girl damn song', 'song make wan na cry smile miss some1 never met', 'miss someone listen song', 'alone listening end 2023', 'miss 1d day', 'songe make cry sister sing together', 'goin tonight change something red mother doesnt like kind dress everything never shes showin drivin fast moon breakin hair shes headin somethin wont forget havin regret really want gettin older baby ive thinkin lately ever drive crazy fast night change everything youve ever dreamed disappearing wake there nothing afraid even night change never change chasing tonight doubt runnin round head he waitin hide behind cigarette heart beatin loud doesnt want stop movin fast moon lightin skin shes fallin doesnt even know yet havin regret really want gettin older baby ive thinkin lately ever drive crazy fast night change everything youve ever dreamed disappearing wake there nothing afraid even night change never change goin tonight change something red mother doesnt like kind dress reminds missin piece innocence lost gettin older baby ive thinkin lately ever drive crazy fast night change everything youve ever dreamed disappearing wake there nothing afraid even night change never change baby never change baby never change', 'miss moment', 'recent directioner gold im sad late bandwagon']\n",
      "15432\n"
     ]
    }
   ],
   "source": [
    "def apply_algorithm():\n",
    "    global positive_rate_label, negative_rate_label, neutral_rate_label, algorithm_selector\n",
    "    input_download(video_id_entry)\n",
    "    data=import_data_read()\n",
    "    data1=data_preprocessing(data)\n",
    "    rate,dataval = sentimental_analysis(data1)\n",
    "    print(dataval)\n",
    "    positive=(rate[0])\n",
    "    negative=(rate[1])\n",
    "    neutral=(rate[2])\n",
    "    \n",
    "    positive_rate_label.config(text=f\"Positive Rate-2: {rate[0]:.2f}%\")\n",
    "    negative_rate_label.config(text=f\"Negative Rate-0: {rate[1]:.2f}%\")\n",
    "    neutral_rate_label.config(text=f\"Neutral Rate-1: {rate[2]:.2f}%\")\n",
    "    \n",
    "    data2 = return_data(data1) \n",
    "    print(data2)\n",
    "    corpus,final=sampling(data2)\n",
    "    global img_label\n",
    "    \n",
    "    results=Classifiers(corpus,final) \n",
    "    selected_algorithms = algorithm_selector.curselection()\n",
    "    for index in selected_algorithms:\n",
    "        algorithm = algorithms[index]\n",
    "        frame = ttk.Frame(result_notebook)\n",
    "        result_notebook.add(frame, text=algorithm)  # Add only selected frames\n",
    "        labels[algorithm] = []  \n",
    "        for idx, (metric, value) in enumerate(results[algorithm].items()):\n",
    "            label = tk.Label(frame, text=f\"{metric}: {value}%\", font=label_font, bg=\"#ffffff\", fg=\"#ef233c\")\n",
    "            label.pack(anchor=tk.W, padx=100)\n",
    "            labels[algorithm].append(label) \n",
    "                \n",
    "    fig, ax = plt.subplots(figsize=(4, 2)) \n",
    "    data2.groupby('Sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('plot_image.png') \n",
    "    plt.close() \n",
    "    \n",
    "    if img_label:\n",
    "        image = Image.open('plot_image.png')\n",
    "        img = ImageTk.PhotoImage(image)\n",
    "        img_label.configure(image=img)\n",
    "        img_label.image = img\n",
    "    else:\n",
    "        image = Image.open('plot_image.png')\n",
    "        img = ImageTk.PhotoImage(image)\n",
    "        img_label = tk.Label(root, image=img)\n",
    "        img_label.image = img\n",
    "        img_label.pack(padx=10)\n",
    "    result_notebook.pack(fill='both', expand=True, padx=100, pady=20, side=tk.BOTTOM)\n",
    "\n",
    "def clear_labels(algo=None):\n",
    "    if algo:\n",
    "        if algo in labels:\n",
    "            for label in labels[algo]:\n",
    "                label.grid_forget()\n",
    "            del labels[algo]\n",
    "    else:\n",
    "        for algorithm, label_list in labels.items():\n",
    "            for label in label_list:\n",
    "                label.grid_forget()\n",
    "        labels.clear()\n",
    "    result_notebook.pack_forget()\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Sentiment Analysis\")\n",
    "root.geometry(\"800x800\")\n",
    "root.configure(bg=\"#1e1e1e\")\n",
    "\n",
    "style = ttk.Style()\n",
    "style.theme_use(\"clam\")\n",
    "\n",
    "title_font = (\"Arial\", 24, \"bold\")\n",
    "label_font = (\"Arial\", 16)\n",
    "button_font = (\"Arial\", 16)\n",
    "\n",
    "padding_y = 10\n",
    "padding_x = 20\n",
    "margin_y = 5\n",
    "\n",
    "title_label = tk.Label(root, text=\"YouTube Sentiment Analysis\", font=title_font, fg=\"#ff0000\", bg=\"#1e1e1e\")\n",
    "title_label.pack(pady=padding_y)\n",
    "\n",
    "video_id_label = tk.Label(root, text=\"Enter Video ID:\", font=label_font, bg=\"#1e1e1e\", fg=\"#ffffff\")\n",
    "video_id_label.pack(pady=padding_y)\n",
    "video_id_entry = tk.Entry(root, width=30, font=label_font)\n",
    "video_id_entry.pack(pady=margin_y, padx=padding_x)\n",
    "\n",
    "algorithms = [\"Naive Bayes\", \"Random Forest\", \"Logistic Regression\", \"SVM\", \"GradientBoosting\", \"Stacking_Classifier\"]\n",
    "\n",
    "algo_indices = {algo: idx for idx, algo in enumerate(algorithms)}\n",
    "style.configure(\"TListbox\", background=\"#2c3e50\", foreground=\"white\", fieldbackground=\"#34495e\", selectbackground=\"#3498db\", selectforeground=\"white\")\n",
    "\n",
    "style.configure(\"TNotebook.Tab\", font=('Arial', 11, 'bold'), foreground=\"#ffffff\", background=\"#333333\")\n",
    "style.map(\"TNotebook.Tab\", background=[(\"selected\", \"#ff0000\")])\n",
    "\n",
    "algorithm_selector = tk.Listbox(root, selectmode=tk.MULTIPLE, exportselection=False, font=label_font, bg=\"#202020\", fg=\"#ffffff\", selectbackground=\"#ef6248\", selectforeground=\"#ffffff\", highlightthickness=0,height=5)\n",
    "algorithm_selector.insert(tk.END, *algorithms)\n",
    "algorithm_selector.pack(pady=padding_y)\n",
    "\n",
    "positive_rate_label = tk.Label(root, text=\"Positive Rate:\", font=label_font, bg=\"#1e1e1e\", fg=\"#27ae60\")\n",
    "positive_rate_label.pack(padx=5)\n",
    "\n",
    "negative_rate_label = tk.Label(root, text=\"Negative Rate:\", font=label_font, bg=\"#1e1e1e\", fg=\"#d90429\")\n",
    "negative_rate_label.pack(padx=5)\n",
    "\n",
    "neutral_rate_label = tk.Label(root, text=\"Neutral Rate:\", font=label_font, bg=\"#1e1e1e\", fg=\"#2980b9\")\n",
    "neutral_rate_label.pack(padx=5)\n",
    "\n",
    "apply_button = tk.Button(root, text=\"Apply\", command=lambda: [clear_labels(), apply_algorithm()], font=button_font, width=15, bg=\"#ff0000\", fg=\"#ffffff\")\n",
    "apply_button.pack(pady=padding_y)\n",
    "\n",
    "result_notebook = ttk.Notebook(root)\n",
    "result_notebook.pack(fill='both', expand=True, padx=100, pady=20, side=tk.BOTTOM)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b6036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d6937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3cf6fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a1de215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c22af6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e94dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0838afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ca935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f11cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d52c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f755966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b18cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_algorithm():\n",
    "#     video_id=video_id_entry.get()\n",
    "#     data2 = {'Sentiment': ['Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Negative']}\n",
    "#     data2 = pd.DataFrame(data2)     \n",
    "    input_download(video_id_entry)\n",
    "    data=import_data_read()\n",
    "    data1=data_preprocessing(data)\n",
    "    rate = sentimental_analysis(data1)\n",
    "    positive=(rate[0])\n",
    "    negative=(rate[1])\n",
    "    neutral=(rate[2])\n",
    "    \n",
    "    positive_rate_label.config(text=f\"Positive Rate: {rate[0]:.2f}%\")\n",
    "    negative_rate_label.config(text=f\"Negative Rate: {rate[1]:.2f}%\")\n",
    "    neutral_rate_label.config(text=f\"Neutral Rate: {rate[2]:.2f}%\")\n",
    "    \n",
    "    data2 = return_data(data1) \n",
    "    corpus,final=sampling(data2)\n",
    "    fig, ax = plt.subplots(figsize=(4, 2)) \n",
    "    data2.groupby('Sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('plot_image.png') \n",
    "    plt.close() \n",
    "    \n",
    "    image = Image.open('plot_image.png') \n",
    "    img = ImageTk.PhotoImage(image)\n",
    "    img_label = tk.Label(root, image=img)\n",
    "    img_label.image = img \n",
    "    img_label.pack(padx=10)\n",
    "    results=Classifiers(corpus,final)\n",
    "    for algorithm, data in results.items():\n",
    "        frame = frames[algorithm]\n",
    "        for idx, (metric, value) in enumerate(data.items()):\n",
    "            label = tk.Label(frame, text=f\"{metric}: {value}%\", font=label_font, bg=\"#ffffff\", fg=\"#ef233c\")\n",
    "            label.pack(anchor=tk.W, padx=100)\n",
    "            labels[algorithm].append(label)\n",
    "\n",
    "def clear_labels():\n",
    "    for algorithm, label_list in labels.items():\n",
    "        for label in label_list:\n",
    "            label.pack_forget()\n",
    "        labels[algorithm] = []\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Sentiment Analysis\")\n",
    "root.geometry(\"800x800\")\n",
    "root.configure(bg=\"#1e1e1e\")\n",
    "\n",
    "title_font = (\"Arial\", 24, \"bold\")\n",
    "label_font = (\"Arial\", 16)\n",
    "button_font = (\"Arial\", 16)\n",
    "\n",
    "padding_y = 10\n",
    "padding_x = 20\n",
    "margin_y = 5\n",
    "\n",
    "title_label = tk.Label(root, text=\"YouTube Sentiment Analysis\", font=title_font, fg=\"#ff0000\", bg=\"#1e1e1e\")\n",
    "title_label.pack(pady=padding_y)\n",
    "\n",
    "video_id_label = tk.Label(root, text=\"Enter Video ID:\", font=label_font, bg=\"#1e1e1e\", fg=\"#ffffff\")\n",
    "video_id_label.pack(pady=padding_y)\n",
    "video_id_entry = tk.Entry(root, width=30, font=label_font)\n",
    "video_id_entry.pack(pady=margin_y, padx=padding_x)\n",
    "\n",
    "apply_button = tk.Button(root, text=\"Apply\", command=lambda: [clear_labels(), apply_algorithm()], font=button_font, width=15, bg=\"#ff0000\", fg=\"#ffffff\")\n",
    "apply_button.pack(pady=padding_y)\n",
    "\n",
    "\n",
    "positive_rate_label = tk.Label(root, text=\"Positive Rate\", font=label_font, bg=\"#1e1e1e\", fg=\"#27ae60\")\n",
    "positive_rate_label.pack(padx=5)\n",
    "\n",
    "negative_rate_label = tk.Label(root, text=\"Negative Rate\", font=label_font, bg=\"#1e1e1e\", fg=\"#d90429\")\n",
    "negative_rate_label.pack(padx=5)\n",
    "\n",
    "neutral_rate_label = tk.Label(root, text=\"Neutral Rate\", font=label_font, bg=\"#1e1e1e\", fg=\"#2980b9\")\n",
    "neutral_rate_label.pack(padx=5)\n",
    "\n",
    "notebook = ttk.Notebook(root)\n",
    "notebook.pack(fill='both', expand=True, padx=20, pady=20, side=tk.BOTTOM)\n",
    "\n",
    "algorithms = [\"Naive Bayes\", \"Random Forest\", \"Logistic Regression\", \"SVM\", \"GradientBoosting\"]\n",
    "frames = {}\n",
    "for algo in algorithms:\n",
    "    frames[algo] = ttk.Frame(notebook)\n",
    "    notebook.add(frames[algo], text=algo)\n",
    "    \n",
    "style = ttk.Style()\n",
    "style.configure(\"TNotebook.Tab\", padding=[20, 5], font=('Arial', 14), foreground='#211412', background='#ff4848')\n",
    "style.configure(\"TNotebook\", tabposition='n', background='#1e1e1e', borderwidth=0)\n",
    "\n",
    "labels = {algo: [] for algo in algorithms}\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e587013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cc225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97339794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ff4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fc1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d295400b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8527630582891749\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      1749\n",
      "           1       0.95      0.66      0.78      1808\n",
      "           2       0.77      0.94      0.85      1727\n",
      "\n",
      "    accuracy                           0.85      5284\n",
      "   macro avg       0.87      0.86      0.85      5284\n",
      "weighted avg       0.87      0.85      0.85      5284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Naive =MultinomialNB()\n",
    "Naive.fit(training_data, y_train)\n",
    "y_pred = Naive.predict(testing_data)\n",
    "accuracy=accuracy_score(y_pred,y_test)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc1f5a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9912944738834216\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1749\n",
      "           1       0.99      1.00      0.99      1808\n",
      "           2       1.00      0.98      0.99      1727\n",
      "\n",
      "    accuracy                           0.99      5284\n",
      "   macro avg       0.99      0.99      0.99      5284\n",
      "weighted avg       0.99      0.99      0.99      5284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc=SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svc.fit(training_data,y_train)\n",
    "y_pred=svc.predict(testing_data)\n",
    "accuracy=accuracy_score(y_pred,y_test)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fba2c7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6970098410295231\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.56      0.68      1749\n",
      "           1       0.57      0.97      0.72      1808\n",
      "           2       0.90      0.55      0.68      1727\n",
      "\n",
      "    accuracy                           0.70      5284\n",
      "   macro avg       0.78      0.69      0.69      5284\n",
      "weighted avg       0.77      0.70      0.69      5284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    boost_class=GradientBoostingClassifier(n_estimators=10,random_state=3)\n",
    "    boost_class.fit(training_data,y_train)\n",
    "    y_pred=boost_class.predict(testing_data)\n",
    "    accuracy=accuracy_score(y_pred,y_test)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d71167b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9848599545798638\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1749\n",
      "           1       0.98      0.99      0.98      1808\n",
      "           2       0.99      0.97      0.98      1727\n",
      "\n",
      "    accuracy                           0.98      5284\n",
      "   macro avg       0.99      0.98      0.98      5284\n",
      "weighted avg       0.98      0.98      0.98      5284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    logistic_model = LogisticRegression(random_state=42,max_iter=1000)\n",
    "    logistic_model.fit(training_data, y_train)\n",
    "    y_pred = logistic_model.predict(testing_data)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f560a47d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9894019682059046\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1749\n",
      "           1       0.98      0.99      0.99      1808\n",
      "           2       0.99      0.98      0.99      1727\n",
      "\n",
      "    accuracy                           0.99      5284\n",
      "   macro avg       0.99      0.99      0.99      5284\n",
      "weighted avg       0.99      0.99      0.99      5284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    random_forest_model.fit(training_data, y_train)\n",
    "    y_pred = random_forest_model.predict(testing_data)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26e5ddfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    models_report=pd.DataFrame()\n",
    "    models_report['models']=[MultinomialNB(),\n",
    "                  SVC(kernel='linear', C=1.0, random_state=42),\n",
    "                  GradientBoostingClassifier(n_estimators=10,random_state=3),\n",
    "                  LogisticRegression(random_state=42, max_iter=1000),\n",
    "                  RandomForestClassifier(n_estimators=100, random_state=42)]\n",
    "    models=['MultinomialNB()','SVC()','GradientBoostingClassifier()','LogisticRegression()','RandomForestClassifier()']\n",
    "    models_report['model']=models\n",
    "    acc=[]\n",
    "    for model in models_report['models']:\n",
    "      model.fit(training_data, y_train)\n",
    "      y_pred = model.predict(testing_data)\n",
    "      accuracy = accuracy_score(y_test, y_pred)*100\n",
    "      acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be8a58e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>85.276306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC()</td>\n",
       "      <td>99.129447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>69.700984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>98.485995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>98.940197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Models   Accuracy\n",
       "0               MultinomialNB()  85.276306\n",
       "1                         SVC()  99.129447\n",
       "2  GradientBoostingClassifier()  69.700984\n",
       "3          LogisticRegression()  98.485995\n",
       "4      RandomForestClassifier()  98.940197"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    models_report['Accuracy']=acc\n",
    "    Report=pd.DataFrame()\n",
    "    Report['Models']=models\n",
    "    Report['Accuracy']=acc\n",
    "    Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4aaf2125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva Alle\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.9939439818319455\n",
      "Stacking Classifier Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1749\n",
      "           1       0.99      0.99      0.99      1808\n",
      "           2       0.99      0.99      0.99      1727\n",
      "\n",
      "    accuracy                           0.99      5284\n",
      "   macro avg       0.99      0.99      0.99      5284\n",
      "weighted avg       0.99      0.99      0.99      5284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "base_models = [\n",
    "    ('naive_bayes', MultinomialNB()),\n",
    "    ('svm',SVC(kernel='linear', C=1.0, random_state=42)),\n",
    "    ('gradient_boosting',GradientBoostingClassifier(n_estimators=10,random_state=3)),\n",
    "    ('logistic_regression',  LogisticRegression(random_state=42,max_iter=5000)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
    "stacking_classifier.fit(training_data, y_train)\n",
    "y_pred_stacking = stacking_classifier.predict(testing_data)\n",
    "accuracy_stacking = accuracy_score(y_pred_stacking, y_test)\n",
    "classification_rep_stacking = classification_report(y_test, y_pred_stacking)\n",
    "\n",
    "print(\"Stacking Classifier Accuracy:\", accuracy_stacking)\n",
    "print(\"Stacking Classifier Classification Report:\\n\", classification_rep_stacking)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
